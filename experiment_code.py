# -*- coding: utf-8 -*-
"""Experiment Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13RPplM0EFmXEXDwCVDtWYQHzvljBsAAX
"""

# 基础库
!pip install -q transformers==4.45.0 datasets pillow tqdm matplotlib

# 4bit 量化所需库
!pip install -q bitsandbytes accelerate

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# cd 到指定目录
# %cd /content/drive/MyDrive/

# Commented out IPython magic to ensure Python compatibility.
# 克隆评测代码仓库
!git clone https://github.com/MLRM-Halu/MLRM-Halu.git

# 切换到项目文件夹
# %cd MLRM-Halu

# 查看目录结构
!ls -al

import torch
from transformers import AutoTokenizer, AutoModelForVision2Seq, CLIPProcessor, CLIPModel
from PIL import Image
import requests
from tqdm import tqdm

# 其他辅助库
import matplotlib.pyplot as plt
import pandas as pd

from datasets import load_dataset

# 加载感知任务，数据存放在 halu_data.json 中
dataset = load_dataset("LCZZZZ/RH-Bench", data_files="halu_data.json", split="train")
print(dataset[0])  # 将显示 question、image、answer 等字段

from PIL import Image

def load_image_from_path(path: str) -> Image.Image:
    return Image.open(path).convert("RGB")

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

model_id = "llava-hf/llava-1.5-7b-hf"  # 可替换为 liuhaotian/llava-v1.5-7b 等

# 加载模型（FP16），低 CPU 内存模式
model = LlavaForConditionalGeneration.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    # load_in_4bit=True  # 若安装了 bitsandbytes，可取消注释启用4bit量化
).to("cuda")

# 加载处理器
processor = AutoProcessor.from_pretrained(model_id)

def llava_inference(model, processor, image: Image.Image, question: str) -> str:
    """
    使用LLaVA模型生成回答。
    参数:
      - model: LlavaForConditionalGeneration 实例
      - processor: 对应的 AutoProcessor
      - image: PIL Image 对象
      - question: 问题文本
    返回:
      - 模型生成的答案字符串
    """
    # 构建聊天提示，<image> 占位符用于放置图片
    full_prompt = f"USER: <image>\n{question} ASSISTANT:"
    # 编码图像和文本
    inputs = processor(
        text=full_prompt,
        images=image,
        return_tensors="pt"
    ).to(model.device)
    # 生成答案
    output_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)
    # 解码并去除模板前缀
    answer = processor.decode(output_ids[0], skip_special_tokens=True)
    answer = answer.split("ASSISTANT:")[-1].strip()
    return answer

import os
from tqdm import tqdm
from PIL import Image

# 请根据实际情况修改 dataset_root，使其指向包含 per_images 目录的根路径
dataset_root = "/content/RH-Bench"  # 例：如果 halu_data.json 和 per_images 在该路径下

def load_image_from_path(path: str) -> Image.Image:
    """从给定路径加载图片并转换为 RGB 模式"""
    return Image.open(path).convert("RGB")

def evaluate_baseline(inference_fn, model, processor, dataset):
    """
    运行基线评测：
    - inference_fn: 推理函数，例如 llava_inference
    - model: 已加载的模型
    - processor: 对应的处理器
    - dataset: 已加载的 RH-Bench 数据集（含 question、answer、image 字段）
    返回：正确数、幻觉数、总样本数
    """
    correct = 0
    hallucinations = 0
    total = len(dataset)
    for data in tqdm(dataset):
        # halu_data.json 中没有 'image_path' 字段，只有 'image':contentReference[oaicite:0]{index=0}。
        # 需要将相对路径与 dataset_root 拼接为图片的绝对路径。
        image_rel_path = data['image']
        img_path = os.path.join(dataset_root, image_rel_path)
        img = load_image_from_path(img_path)

        question = data['question']
        pred = inference_fn(model, processor, img, question)

        # 答案一致视为正确，否则计为幻觉
        if pred.strip().lower() == data['answer'].strip().lower():
            correct += 1
        else:
            hallucinations += 1
    return correct, hallucinations, total

# 调用示例：
# correct, hallu, total = evaluate_baseline(llava_inference, model, processor, dataset)
# print(f"总样本数：{total}，正确回答数：{correct}，幻觉数：{hallu}")

def llava_backtracking_inference(model, processor, image: Image.Image, question: str) -> str:
    # 第一次推理
    initial_answer = llava_inference(model, processor, image, question)

    # 构建回溯提示：询问答案是否正确，若错误则重新回答
    prompt_bt = (
        f"USER: <image>\n问题：{question}\n"
        f"你之前的回答是：{initial_answer}\n"
        "请严格根据图片内容判断这个回答是否正确。如果正确，请回答“是”；如果不正确，请回答“否”并给出正确答案。"
        " ASSISTANT:"
    )
    inputs = processor(text=prompt_bt, images=image, return_tensors="pt").to(model.device)
    output_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)
    result = processor.decode(output_ids[0], skip_special_tokens=True)
    result = result.split("ASSISTANT:")[-1].strip()

    # 如果回答中包含“是”，则保留初始答案；否则删除“否”并返回剩余部分
    if "是" in result:
        return initial_answer
    else:
        return result.replace("否", "").strip()

import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration

# 假设 model 和 processor 已经按照之前步骤加载：
# model_id = "llava-hf/llava-1.5-7b-hf"
# model = LlavaForConditionalGeneration.from_pretrained(model_id, ...)
# processor = AutoProcessor.from_pretrained(model_id)

from PIL import Image

def llava_inference(model, processor, image: Image.Image, question: str) -> str:
    """
    使用 LLaVA 模型生成回答。
    参数:
      - model: LlavaForConditionalGeneration 实例
      - processor: 对应的 AutoProcessor
      - image: PIL Image 对象
      - question: 问题文本
    返回:
      - 模型生成的答案字符串
    """
    # 构建聊天提示
    full_prompt = f"USER: <image>\n{question} ASSISTANT:"
    inputs = processor(
        text=full_prompt,
        images=image,
        return_tensors="pt"
    ).to(model.device)
    # 生成答案
    output_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)
    answer = processor.decode(output_ids[0], skip_special_tokens=True)
    # 去掉模板前缀
    answer = answer.split("ASSISTANT:")[-1].strip()
    return answer

def llava_backtracking_inference(model, processor, image: Image.Image, question: str) -> str:
    """
    基于回溯验证的推理函数：
    先让模型回答问题，再要求其验证并纠错。
    """
    # 第一次推理
    initial_answer = llava_inference(model, processor, image, question)
    # 构建回溯提示
    prompt_bt = (
        f"USER: <image>\n问题：{question}\n"
        f"你之前的回答是：{initial_answer}\n"
        "请严格根据图片内容判断这个回答是否正确。如果正确，请回答“是”；"
        "如果不正确，请回答“否”并给出正确答案。 ASSISTANT:"
    )
    inputs = processor(text=prompt_bt, images=image, return_tensors="pt").to(model.device)
    output_ids = model.generate(**inputs, max_new_tokens=64, do_sample=False)
    result = processor.decode(output_ids[0], skip_special_tokens=True)
    result = result.split("ASSISTANT:")[-1].strip()
    if "是" in result:
        return initial_answer
    else:
        return result.replace("否", "").strip()

# Import necessary libraries
import matplotlib.pyplot as plt
import numpy as np

# Example reasoning lengths; replace with actual token lengths used in your experiments
reasoning_lengths = [20, 40, 60]

# Replace the following lists with the actual accuracy results from your baseline model
baseline_accuracy = [0.50, 0.55, 0.60]
# Replace with actual hallucination rates for your baseline
baseline_hallucination = [0.30, 0.28, 0.25]

# Replace with the actual accuracy results for your backtracking strategy
backtrack_accuracy = [0.60, 0.65, 0.68]
# Replace with actual hallucination rates for the backtracking strategy
backtrack_hallucination = [0.22, 0.20, 0.18]

# Plot accuracy vs reasoning length for both strategies
plt.figure()
plt.plot(reasoning_lengths, baseline_accuracy, marker='o', label='Baseline Accuracy')
plt.plot(reasoning_lengths, backtrack_accuracy, marker='s', label='Backtracking Accuracy')
plt.xlabel('Reasoning length (tokens)')
plt.ylabel('Accuracy')
plt.title('Accuracy vs Reasoning Length')
plt.legend()
plt.show()

# Plot hallucination rate vs reasoning length for both strategies
plt.figure()
plt.plot(reasoning_lengths, baseline_hallucination, marker='o', label='Baseline Hallucination')
plt.plot(reasoning_lengths, backtrack_hallucination, marker='s', label='Backtracking Hallucination')
plt.xlabel('Reasoning length (tokens)')
plt.ylabel('Hallucination Rate')
plt.title('Hallucination Rate vs Reasoning Length')
plt.legend()
plt.show()